import { NextResponse } from "next/server";
import { exec } from "child_process";
import { promisify } from "util";

const execAsync = promisify(exec);

interface GPUInfo {
  available: boolean;
  type: "nvidia-tensor" | "nvidia-cuda" | "cuda-cpu" | "none";
  gpuName: string | null;
  cudaVersion: string | null;
  vram: number | null; // MB
  tensorCores: boolean;
  computeCapability: string | null;
  recommendation: string;
}

async function detectNvidiaGPU(): Promise<GPUInfo> {
  const defaultResult: GPUInfo = {
    available: false,
    type: "none",
    gpuName: null,
    cudaVersion: null,
    vram: null,
    tensorCores: false,
    computeCapability: null,
    recommendation: "–õ–æ–∫–∞–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU (–º–µ–¥–ª–µ–Ω–Ω–æ)",
  };

  try {
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ nvidia-smi
    const { stdout: nvidiaSmi } = await execAsync("nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv,noheader 2>/dev/null", {
      timeout: 5000,
    });

    if (nvidiaSmi && nvidiaSmi.trim()) {
      const lines = nvidiaSmi.trim().split("\n");
      const firstGpu = lines[0].split(",").map(s => s.trim());
      
      const gpuName = firstGpu[0] || "Unknown NVIDIA GPU";
      const vramStr = firstGpu[1] || "";
      const computeCap = firstGpu[2] || "";

      // –ò–∑–≤–ª–µ–∫–∞–µ–º VRAM –≤ MB
      const vramMatch = vramStr.match(/(\d+)/);
      const vram = vramMatch ? parseInt(vramMatch[1]) : null;

      // –ü—Ä–æ–≤–µ—Ä—è–µ–º compute capability –¥–ª—è —Ç–µ–Ω–∑–æ—Ä–Ω—ã—Ö —è–¥–µ—Ä
      // Tensor cores –¥–æ—Å—Ç—É–ø–Ω—ã —Å compute capability 7.0+ (Volta, Turing, Ampere, Hopper)
      const capMatch = computeCap.match(/(\d+)\.(\d+)/);
      let tensorCores = false;
      let computeCapability: string | null = null;
      
      if (capMatch) {
        const major = parseInt(capMatch[1]);
        const minor = parseInt(capMatch[2]);
        computeCapability = `${major}.${minor}`;
        tensorCores = major >= 7;
      }

      // –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø GPU
      let type: GPUInfo["type"] = "nvidia-cuda";
      if (tensorCores) {
        type = "nvidia-tensor";
      }

      // –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä—Å–∏—é CUDA
      let cudaVersion: string | null = null;
      try {
        const { stdout: nvccOut } = await execAsync("nvcc --version 2>/dev/null | grep release");
        const cudaMatch = nvccOut.match(/release (\d+\.\d+)/);
        if (cudaMatch) {
          cudaVersion = cudaMatch[1];
        }
      } catch {
        // nvcc –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –Ω–æ –¥—Ä–∞–π–≤–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç
      }

      // –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
      let recommendation = "";
      if (tensorCores) {
        recommendation = `üöÄ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ! ${gpuName} —Å —Ç–µ–Ω–∑–æ—Ä–Ω—ã–º–∏ —è–¥—Ä–∞–º–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç—ã`;
      } else if (vram && vram >= 6000) {
        recommendation = `‚úÖ –•–æ—Ä–æ—à–æ. ${gpuName} —Å ${Math.round(vram/1024)}GB VRAM`;
      } else if (vram && vram >= 4000) {
        recommendation = `‚ö†Ô∏è –ü—Ä–∏–µ–º–ª–µ–º–æ. ${gpuName} –¥–ª—è –º–∞–ª—ã—Ö –º–æ–¥–µ–ª–µ–π`;
      } else {
        recommendation = `‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ. ${gpuName} —Ç–æ–ª—å–∫–æ –¥–ª—è –º–∞–ª—ã—Ö –º–æ–¥–µ–ª–µ–π (7B)`;
      }

      return {
        available: true,
        type,
        gpuName,
        cudaVersion,
        vram,
        tensorCores,
        computeCapability,
        recommendation,
      };
    }
  } catch {
    // nvidia-smi –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
  }

  // –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA –Ω–∞ CPU (–Ω–∞–ø—Ä–∏–º–µ—Ä, Intel MKL –∏–ª–∏ AMD ROCm)
  try {
    const { stdout: rocmSmi } = await execAsync("rocm-smi --showname 2>/dev/null", {
      timeout: 5000,
    });
    
    if (rocmSmi && rocmSmi.trim()) {
      return {
        available: true,
        type: "cuda-cpu",
        gpuName: "AMD GPU (ROCm)",
        cudaVersion: "ROCm",
        vram: null,
        tensorCores: false,
        computeCapability: null,
        recommendation: "AMD GPU —á–µ—Ä–µ–∑ ROCm –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è",
      };
    }
  } catch {
    // ROCm –Ω–µ –Ω–∞–π–¥–µ–Ω
  }

  // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ CUDA toolkit –±–µ–∑ GPU
  try {
    const { stdout: nvccOut } = await execAsync("nvcc --version 2>/dev/null | grep release");
    if (nvccOut) {
      const cudaMatch = nvccOut.match(/release (\d+\.\d+)/);
      return {
        available: true,
        type: "cuda-cpu",
        gpuName: null,
        cudaVersion: cudaMatch ? cudaMatch[1] : "unknown",
        vram: null,
        tensorCores: false,
        computeCapability: null,
        recommendation: "CUDA –µ—Å—Ç—å, –Ω–æ GPU –Ω–µ –Ω–∞–π–¥–µ–Ω. –ë—É–¥–µ—Ç CPU",
      };
    }
  } catch {
    // CUDA –Ω–µ –Ω–∞–π–¥–µ–Ω
  }

  return defaultResult;
}

export async function GET() {
  const gpuInfo = await detectNvidiaGPU();

  return NextResponse.json({
    success: true,
    gpu: gpuInfo,
    timestamp: new Date().toISOString(),
  });
}
